{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-09-17T16:46:52.705265Z",
     "start_time": "2025-09-17T16:46:52.076148Z"
    }
   },
   "source": [
    "# download some digit images from MNIST\n",
    "!python -m wget \"https://learnopencv.com/wp-content/uploads/2024/07/mnist_0.jpg\" -o \"mnist_0.jpg\"\n",
    "!python -m wget \"https://learnopencv.com/wp-content/uploads/2024/07/mnist_1.jpg\" -o \"mnist_1.jpg\""
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \u001B[35m\"<frozen runpy>\"\u001B[0m, line \u001B[35m198\u001B[0m, in \u001B[35m_run_module_as_main\u001B[0m\n",
      "  File \u001B[35m\"<frozen runpy>\"\u001B[0m, line \u001B[35m88\u001B[0m, in \u001B[35m_run_code\u001B[0m\n",
      "  File \u001B[35m\"E:\\py_project\\TorchStudy\\.venv\\Lib\\site-packages\\wget.py\"\u001B[0m, line \u001B[35m568\u001B[0m, in \u001B[35m<module>\u001B[0m\n",
      "    filename = download(args[0], out=options.output)\n",
      "  File \u001B[35m\"E:\\py_project\\TorchStudy\\.venv\\Lib\\site-packages\\wget.py\"\u001B[0m, line \u001B[35m526\u001B[0m, in \u001B[35mdownload\u001B[0m\n",
      "    (tmpfile, headers) = \u001B[31mulib.urlretrieve\u001B[0m\u001B[1;31m(binurl, tmpfile, callback)\u001B[0m\n",
      "                         \u001B[31m~~~~~~~~~~~~~~~~\u001B[0m\u001B[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001B[0m\n",
      "  File \u001B[35m\"C:\\Users\\31987\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\urllib\\request.py\"\u001B[0m, line \u001B[35m214\u001B[0m, in \u001B[35murlretrieve\u001B[0m\n",
      "    with contextlib.closing(\u001B[31murlopen\u001B[0m\u001B[1;31m(url, data)\u001B[0m) as fp:\n",
      "                            \u001B[31m~~~~~~~\u001B[0m\u001B[1;31m^^^^^^^^^^^\u001B[0m\n",
      "  File \u001B[35m\"C:\\Users\\31987\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\urllib\\request.py\"\u001B[0m, line \u001B[35m189\u001B[0m, in \u001B[35murlopen\u001B[0m\n",
      "    return \u001B[31mopener.open\u001B[0m\u001B[1;31m(url, data, timeout)\u001B[0m\n",
      "           \u001B[31m~~~~~~~~~~~\u001B[0m\u001B[1;31m^^^^^^^^^^^^^^^^^^^^\u001B[0m\n",
      "  File \u001B[35m\"C:\\Users\\31987\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\urllib\\request.py\"\u001B[0m, line \u001B[35m495\u001B[0m, in \u001B[35mopen\u001B[0m\n",
      "    response = meth(req, response)\n",
      "  File \u001B[35m\"C:\\Users\\31987\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\urllib\\request.py\"\u001B[0m, line \u001B[35m604\u001B[0m, in \u001B[35mhttp_response\u001B[0m\n",
      "    response = self.parent.error(\n",
      "        'http', request, response, code, msg, hdrs)\n",
      "  File \u001B[35m\"C:\\Users\\31987\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\urllib\\request.py\"\u001B[0m, line \u001B[35m533\u001B[0m, in \u001B[35merror\u001B[0m\n",
      "    return \u001B[31mself._call_chain\u001B[0m\u001B[1;31m(*args)\u001B[0m\n",
      "           \u001B[31m~~~~~~~~~~~~~~~~\u001B[0m\u001B[1;31m^^^^^^^\u001B[0m\n",
      "  File \u001B[35m\"C:\\Users\\31987\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\urllib\\request.py\"\u001B[0m, line \u001B[35m466\u001B[0m, in \u001B[35m_call_chain\u001B[0m\n",
      "    result = func(*args)\n",
      "  File \u001B[35m\"C:\\Users\\31987\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\urllib\\request.py\"\u001B[0m, line \u001B[35m613\u001B[0m, in \u001B[35mhttp_error_default\u001B[0m\n",
      "    raise HTTPError(req.full_url, code, msg, hdrs, fp)\n",
      "\u001B[1;35murllib.error.HTTPError\u001B[0m: \u001B[35mHTTP Error 403: Forbidden\u001B[0m\n",
      "Traceback (most recent call last):\n",
      "  File \u001B[35m\"<frozen runpy>\"\u001B[0m, line \u001B[35m198\u001B[0m, in \u001B[35m_run_module_as_main\u001B[0m\n",
      "  File \u001B[35m\"<frozen runpy>\"\u001B[0m, line \u001B[35m88\u001B[0m, in \u001B[35m_run_code\u001B[0m\n",
      "  File \u001B[35m\"E:\\py_project\\TorchStudy\\.venv\\Lib\\site-packages\\wget.py\"\u001B[0m, line \u001B[35m568\u001B[0m, in \u001B[35m<module>\u001B[0m\n",
      "    filename = download(args[0], out=options.output)\n",
      "  File \u001B[35m\"E:\\py_project\\TorchStudy\\.venv\\Lib\\site-packages\\wget.py\"\u001B[0m, line \u001B[35m526\u001B[0m, in \u001B[35mdownload\u001B[0m\n",
      "    (tmpfile, headers) = \u001B[31mulib.urlretrieve\u001B[0m\u001B[1;31m(binurl, tmpfile, callback)\u001B[0m\n",
      "                         \u001B[31m~~~~~~~~~~~~~~~~\u001B[0m\u001B[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001B[0m\n",
      "  File \u001B[35m\"C:\\Users\\31987\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\urllib\\request.py\"\u001B[0m, line \u001B[35m214\u001B[0m, in \u001B[35murlretrieve\u001B[0m\n",
      "    with contextlib.closing(\u001B[31murlopen\u001B[0m\u001B[1;31m(url, data)\u001B[0m) as fp:\n",
      "                            \u001B[31m~~~~~~~\u001B[0m\u001B[1;31m^^^^^^^^^^^\u001B[0m\n",
      "  File \u001B[35m\"C:\\Users\\31987\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\urllib\\request.py\"\u001B[0m, line \u001B[35m189\u001B[0m, in \u001B[35murlopen\u001B[0m\n",
      "    return \u001B[31mopener.open\u001B[0m\u001B[1;31m(url, data, timeout)\u001B[0m\n",
      "           \u001B[31m~~~~~~~~~~~\u001B[0m\u001B[1;31m^^^^^^^^^^^^^^^^^^^^\u001B[0m\n",
      "  File \u001B[35m\"C:\\Users\\31987\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\urllib\\request.py\"\u001B[0m, line \u001B[35m495\u001B[0m, in \u001B[35mopen\u001B[0m\n",
      "    response = meth(req, response)\n",
      "  File \u001B[35m\"C:\\Users\\31987\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\urllib\\request.py\"\u001B[0m, line \u001B[35m604\u001B[0m, in \u001B[35mhttp_response\u001B[0m\n",
      "    response = self.parent.error(\n",
      "        'http', request, response, code, msg, hdrs)\n",
      "  File \u001B[35m\"C:\\Users\\31987\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\urllib\\request.py\"\u001B[0m, line \u001B[35m533\u001B[0m, in \u001B[35merror\u001B[0m\n",
      "    return \u001B[31mself._call_chain\u001B[0m\u001B[1;31m(*args)\u001B[0m\n",
      "           \u001B[31m~~~~~~~~~~~~~~~~\u001B[0m\u001B[1;31m^^^^^^^\u001B[0m\n",
      "  File \u001B[35m\"C:\\Users\\31987\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\urllib\\request.py\"\u001B[0m, line \u001B[35m466\u001B[0m, in \u001B[35m_call_chain\u001B[0m\n",
      "    result = func(*args)\n",
      "  File \u001B[35m\"C:\\Users\\31987\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\urllib\\request.py\"\u001B[0m, line \u001B[35m613\u001B[0m, in \u001B[35mhttp_error_default\u001B[0m\n",
      "    raise HTTPError(req.full_url, code, msg, hdrs, fp)\n",
      "\u001B[1;35murllib.error.HTTPError\u001B[0m: \u001B[35mHTTP Error 403: Forbidden\u001B[0m\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T16:52:02.868060Z",
     "start_time": "2025-09-17T16:51:56.384837Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2"
   ],
   "id": "2b45c2580136a532",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T16:52:08.888248Z",
     "start_time": "2025-09-17T16:52:08.884581Z"
    }
   },
   "cell_type": "code",
   "source": "print(\"torch version : {}\".format(torch.__version__))",
   "id": "5297159617bbabc0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version : 2.8.0+cu128\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 1. Converting Images to Batched tensors\n",
    "\n",
    "An image is made up of pixel arrays that represent the intensity of pixels in grayscale or the color values in RGB format.\n",
    "\n",
    "When working with deep learning models, it's often necessary to convert these images into tensors, which are the primary data structures used in PyTorch for handling and processing data.\n",
    "\n",
    "* **Tensors**: In PyTorch, tensors are multi-dimensional arrays similar to NumPy arrays, but with additional capabilities for GPU acceleration and automatic differentiation. Tensors are the fundamental building blocks for representing data and parameters in neural networks.\n",
    "* **Batches**: Batching is a technique where multiple data samples (images, in this case) are grouped together into a single tensor. This allows efficient processing of multiple samples simultaneously, to take advantage of the parallel processing capabilities of modern hardware."
   ],
   "id": "c27b458260d05f7c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T16:54:04.362725Z",
     "start_time": "2025-09-17T16:54:04.183300Z"
    }
   },
   "cell_type": "code",
   "source": [
    "digit_0_array_og = cv2.imread(\"mnist_0.jpg\")\n",
    "digit_1_array_og = cv2.imread(\"mnist_1.jpg\")\n",
    "\n",
    "digit_0_array_gray = cv2.imread(\"mnist_0.jpg\",cv2.IMREAD_GRAYSCALE )\n",
    "digit_1_array_gray = cv2.imread(\"mnist_1.jpg\",cv2.IMREAD_GRAYSCALE )\n",
    "\n",
    "# Visualize the image\n",
    "\n",
    "fig, axs = plt.subplots(1,2, figsize=(10,5))\n",
    "\n",
    "\n",
    "axs[0].imshow(digit_0_array_og, cmap='gray',interpolation='none')\n",
    "axs[0].set_title(\"Digit 0 Image\")\n",
    "axs[0].axis('off')\n",
    "\n",
    "axs[1].imshow(digit_1_array_og, cmap=\"gray\", interpolation = 'none')\n",
    "axs[1].set_title(\"Digit 1 Image\")\n",
    "axs[1].axis('off')\n",
    "\n",
    "plt.show()"
   ],
   "id": "d4a8c91d2781b71f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 2 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T16:59:07.088224Z",
     "start_time": "2025-09-17T16:59:07.084983Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Numpy array with three channels\n",
    "print(\"Image array shape: \",digit_0_array_og.shape)\n",
    "print(f\"Min pixel value:{np.min(digit_0_array_og)} ; Max pixel value : {np.max(digit_0_array_og)}\")"
   ],
   "id": "4871bd4153be0dd0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image array shape:  (28, 28, 3)\n",
      "Min pixel value:0 ; Max pixel value : 255\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T16:59:33.328482Z",
     "start_time": "2025-09-17T16:59:33.323316Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# We will have a look at 28x28 single channel image's pixel values\n",
    "digit_0_array_gray"
   ],
   "id": "a34d7fd1d101067f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0,   0,   7,   1,   0,   3,   0,  18,   0,   3,   0,\n",
       "          0,   3,   0,   0,   9,   0,   2,   0,  11,   0,   1,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   7,   7,   0,   0,   0,   0,   0,   2,   8,   0,   4,   0,\n",
       "          0,   0,   6,   4,   0,   2,   3,   2,   0,   0,  11,   0,   0,\n",
       "          0,   0],\n",
       "       [  1,   0,   2,   6,   4,   9,   9,   0,   0,   2,   0,   3,   1,\n",
       "         15,   0,   2,  16,   0,   2,   7,   0,   0,  22,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [ 12,   0,   0,   8,   1,   0,   0,   0,   2,   0,   0,   0,   0,\n",
       "         14,   0,   0,   0,   7,   9,   0,   7,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   3,   5,   0,   0,   4,  16,   0,  10,  14,   6,  29,\n",
       "        122, 182, 255, 255, 152,  66,  26,   0,   0,  15,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   7,   5,   0,   9,  12,   0,   0,   4,   0,  49, 184, 255,\n",
       "        255, 232, 255, 255, 231, 246, 227,  64,   0,   0,   4,   0,   0,\n",
       "          0,   0],\n",
       "       [  2,  15,   0,   0,   6,   0,   0,  13,   6, 148, 241, 255, 248,\n",
       "        236, 194, 151, 192, 253, 252, 244, 231, 121,   5,   6,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   7,   0,   0,   7,   0,  51, 191, 223, 254, 247, 248, 148,\n",
       "         30,   0,   6,  24,   0,  32, 116, 235, 255, 166,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   4,   0,   0,  10,   0, 132, 246, 255, 247, 199,  77,   0,\n",
       "          0,   2,   0,   0,   7,   0,   0,  60, 238, 226,  46,   2,   0,\n",
       "          1,   5],\n",
       "       [  0,   0,   4,   0,  14,  59, 203, 255, 255, 201,  45,   1,  15,\n",
       "          0,   0,   1,   0,   4,   2,   0,   0, 137, 246, 169,   7,   0,\n",
       "          0,   4],\n",
       "       [  2,   0,   9,   0,   4, 127, 252, 252, 198,  32,   0,   0,   0,\n",
       "          5,   0,   3,   4,   0,   0,   1,   5,  78, 255, 222,  16,   1,\n",
       "          0,   3],\n",
       "       [  5,   0,   8,   0,   0, 150, 254, 247,  46,   7,   0,   8,   9,\n",
       "          0,   4,   3,   3,   0,   0,   4,   0,  19, 248, 254,  25,   4,\n",
       "          0,   2],\n",
       "       [  0,   1,   2,   0,   1, 150, 246, 255,  17,   9,   0,   4,   0,\n",
       "          0,   9,   0,   0,   0,   0,   6,   0,  46, 254, 255,  30,   5,\n",
       "          0,   2],\n",
       "       [  0,   1,   0,   0,   6, 139, 241, 251,  43,   0,  15,   6,   0,\n",
       "          9,   7,   0,   0,   0,   0,   3,  19, 120, 255, 240,  31,   5,\n",
       "          0,   3],\n",
       "       [  2,   0,   0,   2,   1, 133, 249, 197,   0,  27,   0,   0,  18,\n",
       "          0,   0,   8,   0,   7,   1,   0,   0, 153, 245, 255,  29,   3,\n",
       "          0,   5],\n",
       "       [  6,   0,   0,   4,   0, 142, 255, 156,   1,   0,   4,   0,   0,\n",
       "          6,   0,   0,   0,   3,   0,  10,  45, 245, 255, 250,  26,   2,\n",
       "          0,   6],\n",
       "       [  0,   0,   0,   0,   5, 152, 239,  63,   0,  14,   0,   4,   0,\n",
       "          1,   8,   0,   9,   0,   0,   5, 236, 255, 255, 152,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   9,  11,   0,   1, 126, 255,  59,   0,   1,   0,   5,   0,\n",
       "          1,   2,   0,   0,   0,  27, 169, 255, 247, 217,  26,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   2,   0,  25,  97, 248,  83,   7,   0,   6,   0,   1,\n",
       "          0,   0,   0,   5,  47, 171, 255, 243, 255, 148,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  2,   0,  10,   0,   0,  13, 219, 255,  15,   0,  11,   0,   9,\n",
       "          6,   0,   7,  97, 239, 249, 243, 255, 163,  46,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [ 15,   0,   0,   2,   6,   0, 100, 232, 246, 166, 104,  24,  32,\n",
       "         72, 128, 180, 245, 247, 255, 255, 174,   4,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   8,   3,   0,   2,  63, 226, 254, 248, 255, 246, 255,\n",
       "        255, 255, 255, 255, 255, 237,  88,  13,  13,   0,  17,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   2,   0,   0,   0,   0,   0,  18, 199, 235, 250, 255, 255,\n",
       "        255, 255, 242, 255, 169,  43,  18,   0,   0,  11,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  2,   5,   0,   4,   1,   5,   8,   0,   9, 104, 169, 241, 248,\n",
       "        255, 247, 220,  95,  10,   7,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0]], dtype=uint8)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1.1. Convert Numpy array to Torch tensors",
   "id": "45908a78650b1540"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T17:01:03.475153Z",
     "start_time": "2025-09-17T17:01:03.375370Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Convert the images to PyTorch tensors and normalize\n",
    "img_tensor_0 = torch.tensor(digit_0_array_og, dtype=torch.float32) / 255.0\n",
    "img_tensor_1 = torch.tensor(digit_1_array_og, dtype=torch.float32) / 255.0\n",
    "\n",
    "print(\"Shape of Normalised Digit 0 Tensor: \", img_tensor_0.shape)\n",
    "print(f\"Normalised Min pixel value: {torch.min(img_tensor_0)} ; Normalised Max pixel value : {torch.max(img_tensor_0)}\")\n",
    "\n",
    "plt.imshow(img_tensor_0,cmap=\"gray\")\n",
    "plt.title(\"Normalised Digit 0 Image\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ],
   "id": "678c88d6d498d500",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Normalised Digit 0 Tensor:  torch.Size([28, 28, 3])\n",
      "Normalised Min pixel value: 0.0 ; Normalised Max pixel value : 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1.2. Creating Input Batch",
   "id": "7ad9c18b36f92175"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T17:02:08.103776Z",
     "start_time": "2025-09-17T17:02:08.099593Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batch_tensor = torch.stack([img_tensor_0, img_tensor_1])\n",
    "\n",
    "# In PyTorch the forward pass of input images to the model is expected to have a batch_size > 1\n",
    "print(\"Batch Tensor Shape:\", batch_tensor.shape)"
   ],
   "id": "d1b2d7464026bbff",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Tensor Shape: torch.Size([2, 28, 28, 3])\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Additionally in PyTorch, image tensors typically follow the shape convention **[N\n",
    ",C ,H ,W]** unlike tensorflow which follows [N, H, W, C].\n",
    "\n",
    "Therefore, we need to bring the color channel to the second dimension. This can be achieved using either `torch.view()` or `torch.permute()`."
   ],
   "id": "867e9a6df90e2ebb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T17:02:58.016636Z",
     "start_time": "2025-09-17T17:02:57.995839Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batch_input = batch_tensor.permute(0,3,1,2)\n",
    "print(\"Batch Tensor Shape:\", batch_input.shape)"
   ],
   "id": "26fdfcd0ddb41a56",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Tensor Shape: torch.Size([2, 3, 28, 28])\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 2. Introduction to Tensors and its Operations\n",
    "\n",
    "We have seen the importance of tensors, now will understand it from ground up.\n",
    "Tensor is simply a fancy name given to matrices. If you are familiar with NumPy arrays, understanding and using PyTorch Tensors will be very easy. A scalar value is represented by a 0-dimensional Tensor. Similarly, a column/row matrix is represented using a 1-D Tensor and so on. Some examples of Tensors with different dimensions are shown for you to visualize and understand.\n",
    "\n",
    "<img src=https://learnopencv.com/wp-content/uploads/2019/05/PyTorch-Tensors.jpg width = 400 height=350>\n",
    "\n",
    "## 2.1. Construct your first Tensor\n",
    "\n",
    "Let’s see how we can create a PyTorch Tensor."
   ],
   "id": "6bde579987552825"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T17:03:48.225276Z",
     "start_time": "2025-09-17T17:03:48.185390Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create a Tensor with just ones in a column\n",
    "a = torch.ones(5)\n",
    "# Print the tensor we created\n",
    "print(a)\n",
    "\n",
    "# Create a Tensor with just zeros in a column\n",
    "b = torch.zeros(5)\n",
    "print(b)"
   ],
   "id": "573b2a8ffb15b63",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.])\n",
      "tensor([0., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We can similarly create Tensor with custom values as shown below.",
   "id": "bd1b1c32cdeec7b0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T17:04:22.140366Z",
     "start_time": "2025-09-17T17:04:22.126594Z"
    }
   },
   "cell_type": "code",
   "source": [
    "c = torch.tensor([1.0, 2.0, 3.0, 4.0, 5.0])\n",
    "print(c)"
   ],
   "id": "ec7c4adf122f4dc8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3., 4., 5.])\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "In all the above cases, we have created vectors or Tensors of dimension 1. Now, let’s create some tensors of higher dimension.",
   "id": "ae00eec5315047f1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T17:04:49.992534Z",
     "start_time": "2025-09-17T17:04:49.985767Z"
    }
   },
   "cell_type": "code",
   "source": [
    "d = torch.zeros(3,2)\n",
    "print(d)\n",
    "\n",
    "e = torch.ones(3,2)\n",
    "print(e)\n",
    "\n",
    "f = torch.tensor([[1.0, 2.0],[3.0, 4.0]])\n",
    "print(f)\n",
    "\n",
    "# 3D Tensor\n",
    "g = torch.tensor([[[1., 2.], [3., 4.]], [[5., 6.], [7., 8.]]])\n",
    "print(g)"
   ],
   "id": "4e5ade2f601e949c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]])\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.]])\n",
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n",
      "tensor([[[1., 2.],\n",
      "         [3., 4.]],\n",
      "\n",
      "        [[5., 6.],\n",
      "         [7., 8.]]])\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We can also find out the shape of a Tensor using **`.shape`** method.",
   "id": "e4406ed2733b2e18"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T17:05:33.904623Z",
     "start_time": "2025-09-17T17:05:33.901317Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f.shape)\n",
    "\n",
    "print(e.shape)\n",
    "\n",
    "print(g.shape)"
   ],
   "id": "7389fe5477ef79ff",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2])\n",
      "torch.Size([3, 2])\n",
      "torch.Size([2, 2, 2])\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2.2. Access an element in Tensor\n",
    "\n",
    "Now that we have created some tensors, let’s see how we can access an element in a Tensor. First let’s see how to do this for 1D Tensor aka vector."
   ],
   "id": "413329d4bb634861"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T17:06:05.574272Z",
     "start_time": "2025-09-17T17:06:05.571053Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Get element at index 2\n",
    "print(c[2])"
   ],
   "id": "b123e655850b4d33",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.)\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "What about 2D or 3D Tensor? Recall what we mentioned about **dimension of a tensor**.\n",
    "\n",
    "To access one particular element in a tensor, we will need to specify indices equal to the dimension of the tensor. That’s why for tensor **`c`** we only had to specify one index."
   ],
   "id": "26d94628cfbaf3ae"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T17:06:25.682997Z",
     "start_time": "2025-09-17T17:06:25.677850Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# All indices starting from 0\n",
    "\n",
    "# Get element at row 1, column 0\n",
    "print(f[1,0])\n",
    "\n",
    "# We can also use the following\n",
    "print(f[1][0])\n",
    "\n",
    "# Similarly for 3D Tensor\n",
    "print(g[1,0,0])\n",
    "print(g[1][0][0])"
   ],
   "id": "53c78fb413079de7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.)\n",
      "tensor(3.)\n",
      "tensor(5.)\n",
      "tensor(5.)\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "But what if you wanted to access one entire row in a 2D Tensor?",
   "id": "a93c5e2aa69e7d10"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T17:06:53.939627Z",
     "start_time": "2025-09-17T17:06:53.901306Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# All elements\n",
    "print(f[:])\n",
    "\n",
    "# All elements from index 1 to 2 (excluding element 3)\n",
    "print(c[1:3])\n",
    "\n",
    "# All elements till index 4 (exclusive)\n",
    "print(c[:4])\n",
    "\n",
    "# First row\n",
    "print(f[0, :])\n",
    "\n",
    "# Second column\n",
    "print(f[:,1])"
   ],
   "id": "c1d01b5e06d2ba59",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n",
      "tensor([2., 3.])\n",
      "tensor([1., 2., 3., 4.])\n",
      "tensor([1., 2.])\n",
      "tensor([2., 4.])\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2.3. Specify data type of elements\n",
    "\n",
    "Whenever we create a tensor, PyTorch decides the data type of the elements of the tensor such that the data type can cover all the elements of the tensor. We can override this by specifying the data type while creating the tensor."
   ],
   "id": "cd6d3f522f0c4195"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T17:07:59.082752Z",
     "start_time": "2025-09-17T17:07:59.071994Z"
    }
   },
   "cell_type": "code",
   "source": [
    "int_tensor = torch.tensor([[1,2,3],[4,5,6]])\n",
    "print(int_tensor.dtype)\n",
    "\n",
    "# What if we changed any one element to floating point number?\n",
    "int_tensor = torch.tensor([[1,2,3],[4.,5,6]])\n",
    "print(int_tensor.dtype)\n",
    "print(int_tensor)\n",
    "\n",
    "# This can be overridden as follows\n",
    "float_tensor = torch.tensor([[1, 2, 3],[4., 5, 6]])\n",
    "int_tensor = float_tensor.type(torch.int64) # set data type\n",
    "print(int_tensor.dtype)\n",
    "print(int_tensor)"
   ],
   "id": "f7477c8d06a18d58",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.int64\n",
      "torch.float32\n",
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.]])\n",
      "torch.int64\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2.4. Tensor to/from NumPy Array\n",
    "\n",
    "We have mentioned several times that PyTorch Tensors and NumPy arrays are pretty similar. This of course demands the question if it’s possible to convert one data structure into another. Let’s see how we can do this."
   ],
   "id": "5c9c96f8657ac441"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T17:09:43.420173Z",
     "start_time": "2025-09-17T17:09:43.406181Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Tensor to Array\n",
    "f_numpy = f.numpy()\n",
    "print(f_numpy)\n",
    "\n",
    "# Array to Tensor\n",
    "h = np.array([[8,7,6,5],[4,3,2,1]])\n",
    "h_tensor = torch.from_numpy(h)\n",
    "print(h_tensor)"
   ],
   "id": "7e3aa293d315f334",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 2.]\n",
      " [3. 4.]]\n",
      "tensor([[8, 7, 6, 5],\n",
      "        [4, 3, 2, 1]])\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2.5. Arithmetic Operations on Tensors\n",
    "\n",
    "Now it’s time for the next step. Let’s see how we can perform arithmetic operations on PyTorch tensors."
   ],
   "id": "6b1c801cb419ac5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T17:10:36.063302Z",
     "start_time": "2025-09-17T17:10:36.011368Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create tensor\n",
    "tensor1 = torch.tensor([[1,2,3],[4,5,6]])\n",
    "tensor2 = torch.tensor([[-1,2,-3],[4,-5,6]])\n",
    "\n",
    "# Addition\n",
    "print(tensor1+tensor2)\n",
    "# We can also use\n",
    "print(torch.add(tensor1,tensor2))\n",
    "\n",
    "# Subtraction\n",
    "print(tensor1-tensor2)\n",
    "# We can also use\n",
    "print(torch.sub(tensor1,tensor2))\n",
    "\n",
    "# Multiplication\n",
    "# Tensor with Scalar\n",
    "print(tensor1 * 2)\n",
    "\n",
    "# Tensor with another tensor\n",
    "# Elementwise Multiplication\n",
    "print(tensor1 * tensor2)\n",
    "\n",
    "# Matrix multiplication\n",
    "tensor3 = torch.tensor([[1,2],[3,4],[5,6]])\n",
    "print(torch.mm(tensor1,tensor3))\n",
    "\n",
    "# Division\n",
    "# Tensor with scalar\n",
    "print(tensor1/2)\n",
    "\n",
    "# Tensor with another tensor\n",
    "# Elementwise division\n",
    "print(tensor1/tensor2)"
   ],
   "id": "cca4cf54bdc0c846",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  4,  0],\n",
      "        [ 8,  0, 12]])\n",
      "tensor([[ 0,  4,  0],\n",
      "        [ 8,  0, 12]])\n",
      "tensor([[ 2,  0,  6],\n",
      "        [ 0, 10,  0]])\n",
      "tensor([[ 2,  0,  6],\n",
      "        [ 0, 10,  0]])\n",
      "tensor([[ 2,  4,  6],\n",
      "        [ 8, 10, 12]])\n",
      "tensor([[ -1,   4,  -9],\n",
      "        [ 16, -25,  36]])\n",
      "tensor([[22, 28],\n",
      "        [49, 64]])\n",
      "tensor([[0.5000, 1.0000, 1.5000],\n",
      "        [2.0000, 2.5000, 3.0000]])\n",
      "tensor([[-1.,  1., -1.],\n",
      "        [ 1., -1.,  1.]])\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2.6. Broadcasting\n",
    "\n",
    "   - `a` is a 1-dimensional tensor with shape \\([ 3 ]\\).\n",
    "   - `b` is a scalar tensor with shape \\([ 1 ]\\).\n",
    "   - When adding `a` and `b`, PyTorch broadcasts `b` to match the shape of `a`, resulting in \\([ 1 + 4, 2 + 4, 3 + 4 ]\\)."
   ],
   "id": "879e65caf9452205"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T17:13:08.939579Z",
     "start_time": "2025-09-17T17:13:08.929390Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create two 1-dimensional tensors\n",
    "a = torch.tensor([1, 2, 3])\n",
    "b = torch.tensor([4])\n",
    "\n",
    "# adding a scalar to a vector\n",
    "result = a + b\n",
    "\n",
    "print(\"Result of Broadcasting:\\n\",result)"
   ],
   "id": "96de604d09130db",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result of Broadcasting:\n",
      " tensor([5, 6, 7])\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "[Broadcasting](https://pytorch.org/docs/stable/notes/broadcasting.html#broadcasting-semantics) allows PyTorch to perform element-wise operations on tensors of\n",
    "   - `a` is a 2-dimensional tensor with shape \\([1, 3]\\).\n",
    "   - `b` is a 2-dimensional tensor with shape \\([3, 1]\\).\n",
    "   - When adding `a` and `b`, PyTorch broadcasts both tensors to the common shape \\([3, 3]\\), resulting in:\n",
    "$$\n",
    "     \\begin{bmatrix}\n",
    "     1+4 & 2+4 & 3+4 \\\\\n",
    "     1+5 & 2+5 & 3+5 \\\\\n",
    "     1+6 & 2+6 & 3+6 \\\\\n",
    "     \\end{bmatrix}\n",
    "$$"
   ],
   "id": "85cbf1332ed3808e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T17:14:21.218614Z",
     "start_time": "2025-09-17T17:14:21.214102Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create two tensors with shapes (1, 3) and (3, 1)\n",
    "a = torch.tensor([[1, 2, 3]])\n",
    "b = torch.tensor([[4], [5], [6]])\n",
    "\n",
    "# adding tensors of different shapes\n",
    "result = a + b\n",
    "print(\"Shape: \", result.shape)\n",
    "print(\"\\n\")\n",
    "print(\"Result of Broadcasting:\\n\", result)"
   ],
   "id": "ef3ccce5989c540d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape:  torch.Size([3, 3])\n",
      "\n",
      "\n",
      "Result of Broadcasting:\n",
      " tensor([[5, 6, 7],\n",
      "        [6, 7, 8],\n",
      "        [7, 8, 9]])\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2.7. CPU v/s GPU Tensor\n",
    "\n",
    "Let’s first see how to create a tensor for GPU."
   ],
   "id": "421420893eb2d292"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T17:15:38.779331Z",
     "start_time": "2025-09-17T17:15:38.582626Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create a tensor for CPU\n",
    "# This will occupy CPU RAM\n",
    "tensor_cpu = torch.tensor([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]], device='cpu')\n",
    "\n",
    "# Create a tensor for GPU\n",
    "# This will occupy GPU RAM\n",
    "tensor_gpu = torch.tensor([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]], device='cuda')"
   ],
   "id": "daedbaf84d8d6a79",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Just like tensor creation, the operations performed for CPU and GPU tensors are also different and consume RAM corresponding to the device specified.",
   "id": "102bc66a371b0db0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T17:17:37.391941Z",
     "start_time": "2025-09-17T17:17:37.386315Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# This uses CPU RAM\n",
    "%time tensor_cpu = tensor_cpu * 5\n",
    "\n",
    "# This uses GPU RAM\n",
    "# Focus on GPU RAM Consumption\n",
    "%time tensor_gpu = tensor_gpu * 5"
   ],
   "id": "5391116f3f8fc2e3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 89.6 μs\n",
      "CPU times: total: 0 ns\n",
      "Wall time: 149 μs\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We can move the GPU tensor to CPU and vice versa as shown below.",
   "id": "6a5dc18f7f750fe"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T17:16:08.404821Z",
     "start_time": "2025-09-17T17:16:08.401155Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Move GPU tensor to CPU\n",
    "tensor_gpu_cpu = tensor_gpu.to(device='cpu')\n",
    "\n",
    "# Move CPU tensor to GPU\n",
    "tensor_cpu_gpu = tensor_cpu.to(device='cuda')"
   ],
   "id": "c49e083c8c1986d2",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 3. Conclusion\n",
    "\n",
    "\n",
    "In this notebook, we started with constructing simple tensors and manipulating them. Next notebook, we will understand the functionality of Torch Autograd and its role in automatic differentiation and gradient computation.\n",
    "\n",
    "\n",
    "In the upcoming notebooks, we will go deeper into backpropagation and various computer vision tasks such as Classification, Segmentation, Object Detection, and Instance Segmentation.\n",
    "\n",
    "Each notebook will provide a detailed explanation and hands-on examples to help you serve as starter notebooks to master the essential tasks in computer vision."
   ],
   "id": "dc9bae6a069e63ac"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "286fa67d6e873a99"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
