{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "通过设计一个基于两个RNN的编码器‐解码器架构，用于序列到序列\n",
    "学习。\\\n",
    "\\\n",
    "具体来说，RNN编码器将长度可变的序列转换为固定形状的上下文变量，然后RNN解码器根据生成的词元和上下文变量按词元生成输出（目标）序列词元。\\\n",
    "然而，即使并非所有输入（源）词元都对解码某个词元都有用，在每个解码步骤中仍使用编码相同的上下文变量。\\\n",
    "有什么方法能改变上下文变量呢？\\\n",
    "\\\n",
    "我们试着从 (Graves, 2013)中找到灵感：\\\n",
    "\t在为给定文本序列生成手写的挑战中，Graves设计了一种可微注意力模型，将文本字符与更长的笔迹对齐，其中对齐方式仅向一个方向移动。受学习对齐想法的启发，Bahdanau等人提出了一个没有严格单向对齐限制的可微注意力模型 (Bahdanau et al., 2014)。\n",
    "\t在预测词元时，如果不是所有输入词元都相关，模型将仅对齐（或参与）输入序列中与当前预测相关的部分。\n",
    "\t这是通过将上下文变量视为注意力集中的输出来实现的。"
   ],
   "id": "7dddebe5db58cb04"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
